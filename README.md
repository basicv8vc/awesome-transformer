# Awesome Transformer [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated list of NLP resources focused on Transformer.

## What is Transformer?

Transformer is a novel neural network architecture which has shown great promise in many natural language processing(NLP) tasks such as machine translation, language modeling, question answering and reading comprehension. 


 <img src="https://github.com/basicv8vc/awesome-transformer/blob/master/imgs/transformer.png" width = "500" height = "700" alt="The Transformer - model architecture " align=center />


More info [here](https://arxiv.org/pdf/1706.03762.pdf).


## Table of Contents

- [Implementations](#implementations)
- [Theory](#theory)
  - [Lectures](#lectures)
  - [Architecture Variants](#architecture-variants)
  - [Surveys](#surveys)
- [Papers](#papers)
  - [Machine Translation](#machine-translation)
  - [Language Modeling](#language-modeling)
  - [Other](#other)
- [Datasets](#datasets)
- [Blogs](#blogs)

## Implementations

|repository| framework|organization|
|---|---|---|
|[tensor2tensor](https://github.com/tensorflow/tensor2tensor)|TensorFlow(official)|Google|
|[allennlp](https://github.com/allenai/allennlp)|PyTorch|[Allen AI](https://github.com/allenai)|
|[annotated-transformer](https://github.com/harvardnlp/annotated-transformer)|PyTorch|[Harvard NLP](http://nlp.seas.harvard.edu/)|
|[attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch)|PyTorch|[jadore801120](https://github.com/jadore801120)|
|[gluon-nlp](https://github.com/dmlc/gluon-nlp)|MXNet|[dmlc](https://github.com/dmlc)|
|[attention-is-all-you-need-keras](https://github.com/Lsdefine/attention-is-all-you-need-keras)|Keras|[Lsdefine](https://github.com/Lsdefine)|
|[attention_is_all_you_need](https://github.com/soskek/attention_is_all_you_need)|Chainer|[soskek](https://github.com/soskek)|

## Theory

### Lectures

|lecture| course |school|
|---|---|---|
|[Transformers and Self-Attention For Generative Models](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf)|CS224n: Natural Language Processing with Deep Learning|Stanford|
|[Self Attention/CNNs](http://phontron.com/class/mtandseq2seq2018/assets/slides/mt-fall2018.chapter9.pdf)|CS 11-731 Machine Translation and Sequence-to-Sequence Models|CMU|
|[Attention](http://phontron.com/class/nn4nlp2018/schedule/attention.html)|CS 11-747 Neural Networks for NLP|CMU|
|[Attention](http://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/slides/lec16.pdf)|CSC 421/2516 Neural Networks and Deep Learning|Toronto|
